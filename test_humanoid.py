# test_humanoid.py - Script de dÃ©couverte Humanoid-v5

import gymnasium as gym
import numpy as np
import time


def test_humanoid_environment(n_episodes=5, render=True):
    """
    Test l'environnement Humanoid-v5 pour comprendre son fonctionnement.

    Args:
        n_episodes: Nombre d'Ã©pisodes Ã  tester
        render: Afficher la visualisation 3D
    """

    # === 1. CRÃ‰ATION DE L'ENVIRONNEMENT ===
    print("ðŸ¤– CrÃ©ation de l'environnement Humanoid-v5...")

    env = gym.make(
        "Humanoid-v5",
        render_mode="human" if render else None,  # Mode visuel
        # Ces paramÃ¨tres sont dans ton yaml
        terminate_when_unhealthy=True,
        healthy_z_range=(1.0, 2.0),
    )

    print(f"âœ… Environnement crÃ©Ã©!\n")

    # === 2. INSPECTION DE L'ESPACE ===
    print("=" * 60)
    print("ðŸ“Š INFORMATIONS SUR L'ENVIRONNEMENT")
    print("=" * 60)

    # Espace d'observation (ce que le robot perÃ§oit)
    obs_space = env.observation_space
    print(f"\nðŸ” OBSERVATIONS:")
    print(f"   - Type: {type(obs_space)}")
    print(f"   - Shape: {obs_space.shape}")
    print(f"   - Min/Max: [{obs_space.low[0]:.2f}, {obs_space.high[0]:.2f}]")
    print(f"   â†’ Le robot reÃ§oit {obs_space.shape[0]} valeurs Ã  chaque step")

    # Espace d'action (ce qu'on peut contrÃ´ler)
    action_space = env.action_space
    print(f"\nðŸŽ® ACTIONS:")
    print(f"   - Type: {type(action_space)}")
    print(f"   - Shape: {action_space.shape}")
    print(f"   - Min/Max: [{action_space.low[0]:.2f}, {action_space.high[0]:.2f}]")
    print(f"   â†’ On contrÃ´le {action_space.shape[0]} articulations")

    # === 3. TEST SUR PLUSIEURS Ã‰PISODES ===
    print("\n" + "=" * 60)
    print("ðŸƒ LANCEMENT DES TESTS")
    print("=" * 60)

    episode_rewards = []
    episode_lengths = []

    for episode in range(n_episodes):
        print(f"\n--- Ã‰pisode {episode + 1}/{n_episodes} ---")

        # RÃ©initialiser l'environnement
        observation, info = env.reset()

        total_reward = 0
        step = 0
        done = False

        # Boucle de l'Ã©pisode
        while not done:
            # ACTION ALÃ‰ATOIRE (pas d'IA pour l'instant)
            # Dans ton entraÃ®nement, c'est le rÃ©seau de neurones qui choisira
            action = env.action_space.sample()

            # STEP: appliquer l'action et observer le rÃ©sultat
            observation, reward, terminated, truncated, info = env.step(action)

            # Accumuler les mÃ©triques
            total_reward += reward
            step += 1

            # L'Ã©pisode est fini si terminated (tombÃ©) ou truncated (timeout)
            done = terminated or truncated

            # Petit ralentissement pour voir l'animation
            if render:
                time.sleep(0.01)

        # Statistiques de l'Ã©pisode
        episode_rewards.append(total_reward)
        episode_lengths.append(step)

        print(f"   âœ… TerminÃ© aprÃ¨s {step} steps")
        print(f"   ðŸ’° Reward total: {total_reward:.2f}")
        print(f"   âŒ Raison: {'TombÃ© (unhealthy)' if terminated else 'Timeout (1000 steps)'}")

    # === 4. STATISTIQUES GLOBALES ===
    print("\n" + "=" * 60)
    print("ðŸ“ˆ RÃ‰SULTATS GLOBAUX (Actions AlÃ©atoires)")
    print("=" * 60)

    print(f"\nðŸŽ¯ Rewards:")
    print(f"   - Moyenne: {np.mean(episode_rewards):.2f}")
    print(f"   - Min: {np.min(episode_rewards):.2f}")
    print(f"   - Max: {np.max(episode_rewards):.2f}")
    print(f"   - Ã‰cart-type: {np.std(episode_rewards):.2f}")

    print(f"\nâ±ï¸  DurÃ©es des Ã©pisodes:")
    print(f"   - Moyenne: {np.mean(episode_lengths):.1f} steps")
    print(f"   - Min: {np.min(episode_lengths)} steps")
    print(f"   - Max: {np.max(episode_lengths)} steps")

    # === 5. INTERPRÃ‰TATION ===
    print("\n" + "=" * 60)
    print("ðŸ§  INTERPRÃ‰TATION")
    print("=" * 60)

    avg_reward = np.mean(episode_rewards)
    avg_length = np.mean(episode_lengths)

    print(f"\nAvec des actions ALÃ‰ATOIRES:")
    print(f"   â†’ Reward moyen: {avg_reward:.1f}")
    print(f"   â†’ Le robot tombe en ~{avg_length:.0f} steps")

    print(f"\nOBJECTIF de l'entraÃ®nement RL:")
    print(f"   â†’ Reward > 5000+ (robot qui marche bien)")
    print(f"   â†’ DurÃ©e maximale (1000 steps sans tomber)")
    print(f"   â†’ Donc environ 50x mieux qu'alÃ©atoire! ðŸš€")

    env.close()

    return episode_rewards, episode_lengths


# === POINT D'ENTRÃ‰E ===
if __name__ == "__main__":
    print("\n" + "ðŸŽ¯" * 30)
    print("TEST HUMANOID-V5 - DÃ‰COUVERTE")
    print("ðŸŽ¯" * 30 + "\n")

    print("ðŸ“Œ Ce script va:")
    print("   1. CrÃ©er l'environnement Humanoid-v5")
    print("   2. Tester avec des actions ALÃ‰ATOIRES")
    print("   3. Te montrer les observations, actions, rewards")
    print("   4. Te donner une baseline pour comparer ton RL aprÃ¨s\n")

    input("Appuie sur ENTER pour commencer...")

    # Lancer le test
    rewards, lengths = test_humanoid_environment(
        n_episodes=15,
        render=True  # Change Ã  False si tu veux juste les chiffres
    )

    print("\nâœ… Test terminÃ©! Tu peux maintenant passer Ã  l'entraÃ®nement RL.")